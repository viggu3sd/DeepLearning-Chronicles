{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 1. Data Preparation: STL-10 and MNIST Setup\n",
    "#############################################\n",
    "\n",
    "# ----- STL-10 (Source Dataset) -----\n",
    "# Define transformations: resize to 96x96, random horizontal flip for augmentation,\n",
    "# conversion to tensor and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10_transform_train = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "stl10_transform_test = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load STL-10 dataset\n",
    "stl10_train = datasets.STL10(root='./data', split='train', download=True, transform=stl10_transform_train)\n",
    "stl10_test = datasets.STL10(root='./data', split='test', download=True, transform=stl10_transform_test)\n",
    "\n",
    "# Data loaders for STL-10\n",
    "stl10_loader_train = DataLoader(stl10_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "stl10_loader_test = DataLoader(stl10_test, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "# ----- MNIST (Target Dataset) -----\n",
    "# MNIST images are 28x28 and grayscale.\n",
    "# We resize them to 96x96 and convert grayscale images to 3 channels.\n",
    "mnist_transform_train = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomRotation(10),  # augmentation: small rotations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "mnist_transform_test = transforms.Compose([\n",
    "    transforms.Resize((96, 96)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform_train)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform_test)\n",
    "\n",
    "# Data loaders for MNIST\n",
    "mnist_loader_train = DataLoader(mnist_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "mnist_loader_test = DataLoader(mnist_test, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 2. Define the CNN Architecture\n",
    "#############################################\n",
    "# We build a simple CNN with a feature extractor (conv layers) and a classifier (fully connected layers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # input channels=3 for RGB images\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # reduce spatial dims from 96x96 -> 48x48\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 48x48 -> 24x24\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)   # 24x24 -> 12x12\n",
    "        )\n",
    "        # Classifier: note 256 channels * 12 * 12 spatial dimensions = 256*144 features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 12 * 12, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the output for the classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 3. Pre-train the CNN on STL-10\n",
    "#############################################\n",
    "# Initialize the model, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pre-training on STL-10...\n",
      "Epoch 1/10 - Loss: 1.8905 | Accuracy: 0.2932\n",
      "Epoch 2/10 - Loss: 1.5018 | Accuracy: 0.4280\n",
      "Epoch 3/10 - Loss: 1.3344 | Accuracy: 0.5114\n",
      "Epoch 4/10 - Loss: 1.1607 | Accuracy: 0.5754\n",
      "Epoch 5/10 - Loss: 1.0649 | Accuracy: 0.6248\n",
      "Epoch 6/10 - Loss: 0.9593 | Accuracy: 0.6492\n",
      "Epoch 7/10 - Loss: 0.8619 | Accuracy: 0.6848\n",
      "Epoch 8/10 - Loss: 0.7683 | Accuracy: 0.7278\n",
      "Epoch 9/10 - Loss: 0.6366 | Accuracy: 0.7642\n",
      "Epoch 10/10 - Loss: 0.5962 | Accuracy: 0.7936\n",
      "Pre-training completed and weights saved.\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # Set the number of epochs as desired\n",
    "\n",
    "print(\"Starting pre-training on STL-10...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in stl10_loader_train:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Save the pre-trained weights\n",
    "torch.save(model.state_dict(), \"stl10_pretrained.pth\")\n",
    "print(\"Pre-training completed and weights saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# 4. Transfer Learning on MNIST: Adaptation Strategies\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility function for training a model on MNIST\n",
    "def train_model(model, optimizer, dataloader, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {running_loss/total:.4f} | Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: computes overall accuracy, precision, recall, F1 score, and confusion matrix.\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 1: Training Linear layers only\n",
      "Epoch 1/5 - Loss: 0.3151 | Accuracy: 0.9063\n",
      "Epoch 2/5 - Loss: 0.1294 | Accuracy: 0.9600\n",
      "Epoch 3/5 - Loss: 0.1031 | Accuracy: 0.9691\n",
      "Epoch 4/5 - Loss: 0.0927 | Accuracy: 0.9709\n",
      "Epoch 5/5 - Loss: 0.0878 | Accuracy: 0.9728\n",
      "\n",
      "Evaluation for Strategy 1 (Linear layers only):\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9849    0.9959    0.9904       980\n",
      "           1     0.9938    0.9930    0.9934      1135\n",
      "           2     0.9902    0.9758    0.9829      1032\n",
      "           3     0.9891    0.9851    0.9871      1010\n",
      "           4     0.9938    0.9766    0.9851       982\n",
      "           5     0.9779    0.9910    0.9844       892\n",
      "           6     0.9885    0.9875    0.9880       958\n",
      "           7     0.9797    0.9864    0.9830      1028\n",
      "           8     0.9688    0.9867    0.9776       974\n",
      "           9     0.9830    0.9732    0.9781      1009\n",
      "\n",
      "    accuracy                         0.9851     10000\n",
      "   macro avg     0.9850    0.9851    0.9850     10000\n",
      "weighted avg     0.9852    0.9851    0.9851     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 976    0    0    0    0    0    1    1    2    0]\n",
      " [   0 1127    1    1    1    0    2    2    1    0]\n",
      " [   1    1 1007    5    1    0    1    8    8    0]\n",
      " [   1    0    1  995    0    6    0    3    4    0]\n",
      " [   2    1    1    0  959    0    5    0    2   12]\n",
      " [   2    0    1    1    0  884    2    1    1    0]\n",
      " [   3    1    0    0    1    4  946    0    3    0]\n",
      " [   1    2    5    1    1    0    0 1014    2    2]\n",
      " [   3    1    1    1    0    2    0    2  961    3]\n",
      " [   2    1    0    2    2    8    0    4    8  982]]\n"
     ]
    }
   ],
   "source": [
    "# -------- Strategy 1: Training Linear Layers Only --------\n",
    "print(\"\\nStrategy 1: Training Linear layers only\")\n",
    "# Initialize a new model and load pre-trained weights.\n",
    "model_linear = SimpleCNN(num_classes=10).to(device)\n",
    "model_linear.load_state_dict(torch.load(\"stl10_pretrained.pth\"))\n",
    "# Freeze the entire feature extractor.\n",
    "for param in model_linear.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define optimizer that only updates classifier parameters.\n",
    "optimizer_linear = optim.Adam(model_linear.classifier.parameters(), lr=0.001)\n",
    "# Train on MNIST\n",
    "train_model(model_linear, optimizer_linear, mnist_loader_train, num_epochs=5)\n",
    "# Evaluate on MNIST test set.\n",
    "print(\"\\nEvaluation for Strategy 1 (Linear layers only):\")\n",
    "evaluate_model(model_linear, mnist_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 2: Freezing the initial few layers of the feature extractor\n",
      "Epoch 1/5 - Loss: 0.2398 | Accuracy: 0.9263\n",
      "Epoch 2/5 - Loss: 0.0725 | Accuracy: 0.9783\n",
      "Epoch 3/5 - Loss: 0.0535 | Accuracy: 0.9842\n",
      "Epoch 4/5 - Loss: 0.0438 | Accuracy: 0.9866\n",
      "Epoch 5/5 - Loss: 0.0386 | Accuracy: 0.9888\n",
      "\n",
      "Evaluation for Strategy 2 (Partial freezing):\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9899    0.9969    0.9934       980\n",
      "           1     0.9930    0.9974    0.9952      1135\n",
      "           2     0.9884    0.9922    0.9903      1032\n",
      "           3     0.9921    0.9921    0.9921      1010\n",
      "           4     0.9939    0.9949    0.9944       982\n",
      "           5     0.9899    0.9910    0.9905       892\n",
      "           6     0.9989    0.9885    0.9937       958\n",
      "           7     0.9826    0.9903    0.9864      1028\n",
      "           8     0.9959    0.9928    0.9943       974\n",
      "           9     0.9950    0.9822    0.9885      1009\n",
      "\n",
      "    accuracy                         0.9919     10000\n",
      "   macro avg     0.9920    0.9918    0.9919     10000\n",
      "weighted avg     0.9919    0.9919    0.9919     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 977    2    0    0    0    0    0    1    0    0]\n",
      " [   0 1132    1    1    0    0    0    1    0    0]\n",
      " [   2    1 1024    0    1    0    0    4    0    0]\n",
      " [   0    0    3 1002    0    3    0    2    0    0]\n",
      " [   0    0    0    0  977    0    0    1    1    3]\n",
      " [   1    0    0    5    0  884    1    0    1    0]\n",
      " [   4    3    0    0    1    3  947    0    0    0]\n",
      " [   0    2    6    0    0    0    0 1018    1    1]\n",
      " [   3    0    2    0    0    0    0    1  967    1]\n",
      " [   0    0    0    2    4    3    0    8    1  991]]\n"
     ]
    }
   ],
   "source": [
    "# -------- Strategy 2: Freezing Initial Few Layers --------\n",
    "print(\"\\nStrategy 2: Freezing the initial few layers of the feature extractor\")\n",
    "# Initialize a new model and load pre-trained weights.\n",
    "model_partial = SimpleCNN(num_classes=10).to(device)\n",
    "model_partial.load_state_dict(torch.load(\"stl10_pretrained.pth\"))\n",
    "# For example, freeze the first convolutional block (the first conv+ReLU+pool)\n",
    "for name, param in model_partial.features.named_parameters():\n",
    "    # We freeze parameters belonging to the first three layers in the features module.\n",
    "    # Adjust the condition depending on your layer naming.\n",
    "    if \"0\" in name or \"1\" in name or \"2\" in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Create optimizer for parameters that are not frozen.\n",
    "optimizer_partial = optim.Adam(filter(lambda p: p.requires_grad, model_partial.parameters()), lr=0.001)\n",
    "# Train on MNIST\n",
    "train_model(model_partial, optimizer_partial, mnist_loader_train, num_epochs=5)\n",
    "# Evaluate on MNIST test set.\n",
    "print(\"\\nEvaluation for Strategy 2 (Partial freezing):\")\n",
    "evaluate_model(model_partial, mnist_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strategy 3: Fine-tuning the entire network\n",
      "Epoch 1/5 - Loss: 0.2654 | Accuracy: 0.9180\n",
      "Epoch 2/5 - Loss: 0.0867 | Accuracy: 0.9738\n",
      "Epoch 3/5 - Loss: 0.0644 | Accuracy: 0.9809\n",
      "Epoch 4/5 - Loss: 0.0526 | Accuracy: 0.9844\n",
      "Epoch 5/5 - Loss: 0.0482 | Accuracy: 0.9857\n",
      "\n",
      "Evaluation for Strategy 3 (Fine-tuning entire network):\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9919    0.9980    0.9949       980\n",
      "           1     0.9947    0.9974    0.9960      1135\n",
      "           2     0.9942    0.9903    0.9922      1032\n",
      "           3     0.9960    0.9881    0.9920      1010\n",
      "           4     0.9830    1.0000    0.9914       982\n",
      "           5     0.9899    0.9922    0.9910       892\n",
      "           6     0.9968    0.9854    0.9911       958\n",
      "           7     0.9836    0.9932    0.9884      1028\n",
      "           8     0.9888    0.9938    0.9913       974\n",
      "           9     0.9980    0.9782    0.9880      1009\n",
      "\n",
      "    accuracy                         0.9917     10000\n",
      "   macro avg     0.9917    0.9917    0.9916     10000\n",
      "weighted avg     0.9917    0.9917    0.9917     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 978    1    0    0    0    0    0    1    0    0]\n",
      " [   0 1132    0    1    1    0    0    1    0    0]\n",
      " [   1    1 1022    0    1    0    1    6    0    0]\n",
      " [   0    0    1  998    0    5    0    3    3    0]\n",
      " [   0    0    0    0  982    0    0    0    0    0]\n",
      " [   0    0    1    3    0  885    2    0    1    0]\n",
      " [   6    2    0    0    4    1  944    0    1    0]\n",
      " [   0    1    3    0    1    0    0 1021    1    1]\n",
      " [   1    1    1    0    0    1    0    1  968    1]\n",
      " [   0    0    0    0   10    2    0    5    5  987]]\n"
     ]
    }
   ],
   "source": [
    "# -------- Strategy 3: Fine-Tuning the Entire Network --------\n",
    "print(\"\\nStrategy 3: Fine-tuning the entire network\")\n",
    "# Initialize a new model and load pre-trained weights.\n",
    "model_finetune = SimpleCNN(num_classes=10).to(device)\n",
    "model_finetune.load_state_dict(torch.load(\"stl10_pretrained.pth\"))\n",
    "# Ensure all parameters are trainable.\n",
    "for param in model_finetune.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer_finetune = optim.Adam(model_finetune.parameters(), lr=0.001)\n",
    "# Train on MNIST\n",
    "train_model(model_finetune, optimizer_finetune, mnist_loader_train, num_epochs=5)\n",
    "# Evaluate on MNIST test set.\n",
    "print(\"\\nEvaluation for Strategy 3 (Fine-tuning entire network):\")\n",
    "evaluate_model(model_finetune, mnist_loader_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
